{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at distance, do we look at urgency score (1-10, 1-5), \n",
    "\n",
    "two vectors, invervention, non-intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "# standard ml libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# deep learning models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline # hugging face's library for \n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import word2vec embedding tool\n",
    "import gensim.downloader as api\n",
    "google_news_vectors = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "# Use a pipeline as a high-level helper\n",
    "pipe = pipeline(\"fill-mask\", model=\"medicalai/ClinicalBERT\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"medicalai/ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# see what it looks like\n",
    "text = \"The patient presented with chest pain and shortness of breath.\"\n",
    "encoded_input = tokenizer(\n",
    "    text,\n",
    "    padding=True,  # Pad shorter sequences to the length of the longest sequence\n",
    "    truncation=True, # Truncate sequences that are longer than the model's maximum input length\n",
    "    return_tensors='pt'  # Return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedMedical():\n",
    "\n",
    "    \"\"\"\n",
    "        Given a single string of text, this embedding model uses a version of ClinicalBERT\n",
    "        called Bio_ClinicalBERT. This embedding tool has been trained on medical data and has a \n",
    "        deep medical vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_medical_embeddings(text):\n",
    "        import torch\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "    \n",
    "        # create pre-trained ClinicalBERT model anem\n",
    "        model = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "        # Load the tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        model = AutoModel.from_pretrained(model)\n",
    "\n",
    "        # Tokenize and encode the text\n",
    "        encoded_input = tokenizer(\n",
    "            text,\n",
    "            max_length = 300,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt' # return pytorch tensor\n",
    "        )\n",
    "\n",
    "        # Get the model outputs\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded_input)\n",
    "\n",
    "        # Access the 'last_hidden_state' attribute to get the embeddings\n",
    "        embedding = output.last_hidden_state[0, 0, :].numpy()\n",
    "         \n",
    "\n",
    "        # 'embeddings' have the shape (batch_size, sequence_length, hidden_size)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Function\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def calculate_distance_metrics(group1, group2):\n",
    "    \"\"\"\n",
    "    Calculate Cohen's D, effect size, and t-test p-value between two sets of embeddings.\n",
    "\n",
    "    Args:\n",
    "        group1 (np.ndarray): Embedding vectors for group 1, shape (n_samples, embedding_dim)\n",
    "        group2 (np.ndarray): Embedding vectors for group 2, shape (n_samples, embedding_dim)\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"cohens_d\": float,\n",
    "            \"effect_size\": str,\n",
    "            \"p_value\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    group1 = np.array(group1)\n",
    "    group2 = np.array(group2)\n",
    "\n",
    "    # Compute the mean vector for each group\n",
    "    group1_mean = group1.mean(axis=0)\n",
    "    group2_mean = group2.mean(axis=0)\n",
    "\n",
    "    # Compute the pooled standard deviation\n",
    "    pooled_std = np.sqrt(((group1.std(axis=0, ddof=1) ** 2) + (group2.std(axis=0, ddof=1) ** 2)) / 2)\n",
    "\n",
    "    # Compute Cohen's D for each dimension and average it\n",
    "    cohens_d_vector = (group1_mean - group2_mean) / pooled_std\n",
    "    cohens_d = np.mean(cohens_d_vector)\n",
    "\n",
    "    # Interpret effect size\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect = \"small\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "\n",
    "    # Compute t-test\n",
    "    t_stat, p_val = ttest_ind(group1, group2, equal_var=False)\n",
    "\n",
    "    return {\n",
    "        \"cohens_d\": cohens_d,\n",
    "        \"effect_size\": effect,\n",
    "        \"p_value\": float(np.mean(p_val))  # average across dimensions\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_group_1 = [EmbedMedical.get_medical_embeddings(txt) for txt in texts_group_1]\n",
    "embedding_group_2 = [EmbedMedical.get_medical_embeddings(txt) for txt in texts_group_2]\n",
    "\n",
    "result = calculate_distance_metrics(embedding_group_1, embedding_group_2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding1 = EmbedMedical.get_medical_embeddings(\"You should rest for 5 days and call us back if you have any concerns or things worsen.\")\n",
    "# embedding2 = EmbedMedical.get_medical_embeddings(\"You should rest for 5 dahave any concerns or things worsen.\")\n",
    "\n",
    "# distance = function(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.54784441e-01, -1.54275864e-01, -3.92404258e-01,  1.79212734e-01,\n",
       "       -1.37097776e-01,  3.45092565e-02,  1.77666575e-01,  2.31356546e-02,\n",
       "        5.67118227e-01, -1.48763403e-01, -1.00918282e-02, -2.31822252e-01,\n",
       "       -6.36753321e-01, -2.10905239e-01, -4.11169350e-01,  2.55564779e-01,\n",
       "        1.86052412e-01, -3.36936563e-01, -4.05172050e-01, -1.82731133e-02,\n",
       "        6.64738491e-02,  3.25947702e-01, -3.71549189e-01, -2.08111003e-01,\n",
       "        2.22359076e-01,  2.04471141e-01,  3.48660111e-01,  3.55831832e-01,\n",
       "        2.29404375e-01,  3.23382646e-01,  4.16044623e-01, -5.44602871e-02,\n",
       "        1.62829310e-02, -2.38350227e-01, -2.17302278e-01,  8.07436258e-02,\n",
       "        1.83334410e-01,  6.73272371e-01,  2.94341333e-03, -1.92279324e-01,\n",
       "       -1.86912775e-01,  1.30772382e-01,  9.20590758e-01, -2.93336838e-01,\n",
       "        2.86309540e-01, -3.54130507e-01,  3.21948886e-01,  3.41619551e-01,\n",
       "       -2.91189611e-01,  1.67546734e-01,  2.28995413e-01,  3.33767831e-01,\n",
       "       -6.14488199e-02, -2.24856526e-01, -2.71338969e-01,  3.38493526e-01,\n",
       "       -1.42938793e-01,  3.70938219e-02, -2.21254379e-02,  4.95552391e-01,\n",
       "       -1.25018641e-01, -3.14589888e-01,  4.07248288e-01, -3.72786194e-01,\n",
       "       -6.57269120e-01,  1.66695639e-01,  5.73570967e-01, -1.36024207e-02,\n",
       "        2.46360064e-01, -1.94495127e-01,  4.87194359e-02, -3.55047435e-01,\n",
       "       -1.35747835e-01,  2.18251720e-02, -2.50941783e-01, -2.56484360e-01,\n",
       "        8.21426809e-01,  3.98969017e-02,  1.29458576e-01,  2.66893178e-01,\n",
       "        1.86042219e-01,  3.51693451e-01, -2.47185200e-01,  5.03597319e-01,\n",
       "        1.72466099e-01, -1.51266843e-01, -2.13861585e-01,  1.99585915e-01,\n",
       "        8.45196098e-02,  1.36468500e-01,  1.41312741e-03, -1.98425114e-01,\n",
       "        1.76807806e-01, -3.46200705e-01, -4.03865784e-01,  2.00385571e-01,\n",
       "       -2.38110833e-02,  2.31120959e-01,  9.00288880e-01, -3.28194983e-02,\n",
       "       -1.20551027e-01, -3.41254532e-01,  1.21149555e-01, -2.64968574e-01,\n",
       "        1.84695631e-01, -1.44721419e-01,  2.05229804e-01, -7.15781748e-01,\n",
       "        9.87699702e-02,  4.81371284e-01,  8.65058899e-02,  1.24271989e-01,\n",
       "       -7.31963068e-02, -2.21400931e-02, -1.45547107e-01, -4.99137118e-02,\n",
       "        1.47522494e-01,  3.49604309e-01,  2.21828043e-01, -5.41452587e-01,\n",
       "        3.68521437e-02, -1.18898168e-01, -9.80442241e-02,  3.42587769e-01,\n",
       "        2.03159347e-01, -1.08019605e-01, -3.25368524e-01, -5.68099976e-01,\n",
       "        2.99482286e-01, -2.88044780e-01, -4.45128858e-01,  1.55638099e-01,\n",
       "       -5.51959760e-02,  6.03574932e-01,  2.86800206e-01,  6.33051753e-01,\n",
       "        1.73431456e-01,  2.77159810e-02, -1.08143175e+00,  5.61419427e-01,\n",
       "       -1.33656099e-01, -1.13163851e-01, -2.98437268e-01,  6.40560389e-02,\n",
       "       -2.90540397e-01, -1.55154333e-01, -4.10156339e-01, -9.79595482e-02,\n",
       "       -7.15450048e-02,  1.24010369e-01, -2.91475087e-01, -3.13120633e-01,\n",
       "        1.84642822e-01,  1.33794546e-01, -2.27166831e-01,  1.62572637e-01,\n",
       "       -2.24737152e-01,  3.09628963e-01,  2.13016644e-01, -4.05221552e-01,\n",
       "       -3.61409128e-01,  3.49734545e-01, -3.05942953e-01, -6.18999973e-02,\n",
       "       -2.75500864e-01, -5.94880618e-02,  3.84709500e-02,  1.50422409e-01,\n",
       "        2.10275114e-01,  6.15615129e-01,  6.44073412e-02,  1.34534910e-01,\n",
       "       -4.72455561e-01,  8.04980278e-01,  1.35631673e-02,  5.02413571e-01,\n",
       "        6.24501407e-02, -3.06358904e-01, -5.09739257e-02, -3.61375690e-01,\n",
       "        2.07888395e-01,  1.79607809e-01,  3.39875147e-02, -4.71030354e-01,\n",
       "       -3.47602904e-01, -3.18305008e-02,  2.02505887e-01, -2.18408838e-01,\n",
       "        3.40201855e-01,  5.70085466e-01,  3.06235701e-01, -2.76412904e-01,\n",
       "        6.90352842e-02, -3.00875567e-02,  2.77865946e-01,  4.34057340e-02,\n",
       "       -1.25726655e-01, -1.96105633e-02, -1.96794331e-01, -3.88750523e-01,\n",
       "       -1.01700991e-01, -1.38381988e-01, -4.03386682e-01, -2.06209458e-02,\n",
       "        2.39652723e-01, -6.42448783e-01,  3.64165187e-01,  3.98139089e-01,\n",
       "        1.04982138e-01, -2.73463905e-01, -6.10075518e-02,  1.38983727e-02,\n",
       "        5.57108223e-03,  2.92782605e-01,  1.79015577e-01, -4.08989310e-01,\n",
       "        3.17999244e-01, -2.57604606e-02,  1.20287091e-02,  2.52989173e-01,\n",
       "        5.67219257e-01, -1.98718347e-02,  6.74543157e-02, -5.57900816e-02,\n",
       "       -1.37095183e-01,  2.62766987e-01,  4.31396216e-01,  4.32982564e-01,\n",
       "       -1.07479408e-01,  2.71064579e-01, -7.15258643e-02, -7.33881414e-01,\n",
       "        1.82501435e-01, -3.03410530e-01, -3.12004030e-01,  1.74202114e-01,\n",
       "        1.25709251e-01,  8.73328447e-02, -1.04436278e-03,  6.78129315e-01,\n",
       "        1.40599266e-01,  1.34990633e-01,  6.31013095e-01, -4.37808260e-02,\n",
       "       -1.06876291e-01, -3.11728179e-01, -3.16253304e-01,  4.42233801e-01,\n",
       "       -1.29055589e-01,  5.90134859e-02,  3.00822526e-01,  3.04352909e-01,\n",
       "       -3.38901222e-01, -1.57041937e-01, -1.21715978e-01, -2.71164119e-01,\n",
       "       -1.37616098e-01,  3.87262404e-02, -5.03751636e-01,  6.33346885e-02,\n",
       "        6.42409548e-02, -1.50604889e-01, -2.24974543e-01,  2.44737029e-01,\n",
       "       -8.12721699e-02,  3.90655026e-02, -1.64309278e-01,  1.34666204e-01,\n",
       "       -4.89622086e-01, -4.45992500e-01, -2.52240866e-01, -3.26737940e-01,\n",
       "       -3.03872108e-01,  2.83833385e-01,  1.10530086e-01,  3.99818331e-01,\n",
       "       -3.54717046e-01, -4.36184853e-01,  2.83449650e-01, -4.59151179e-01,\n",
       "       -3.94851446e-01,  1.01669498e-01, -5.24678767e-01,  7.11537898e-01,\n",
       "       -5.00840284e-02,  7.00727403e-01,  3.13838065e-01,  3.80516350e-01,\n",
       "        1.13181941e-01,  3.00617754e-01,  1.56168118e-02,  1.24297030e-01,\n",
       "       -2.68849045e-01, -7.20179304e-02,  1.75274521e-01, -9.22958031e-02,\n",
       "       -3.19853038e-01,  3.36480290e-01,  1.76117107e-01,  3.86822730e-01,\n",
       "       -7.67552555e-02, -3.85465920e-02,  5.79299808e-01,  1.03601746e-01,\n",
       "       -6.70868307e-02,  7.20249355e-01,  3.25127333e-01,  2.21004725e-01,\n",
       "        3.68869662e-01,  3.08817297e-01, -1.56939462e-01,  2.33863682e-01,\n",
       "       -2.21146256e-01, -1.18825614e-01, -4.10703182e-01,  2.55647629e-01,\n",
       "        2.80848145e-03, -1.18961474e-02,  1.29191905e-01,  2.06574693e-01,\n",
       "       -1.66978538e-02, -4.81339276e-01, -2.33086735e-01, -2.31786773e-01,\n",
       "       -2.71021187e-01, -4.40460145e-01,  4.87981327e-02, -2.38319710e-01,\n",
       "        4.09132004e-01,  3.04695129e-01,  5.60321733e-02,  2.78346926e-01,\n",
       "       -2.82040387e-01, -2.90571749e-01, -1.41744316e-02, -5.78805685e-01,\n",
       "       -6.65597081e-01,  2.90320605e-01, -4.86621648e-01,  4.64892328e-01,\n",
       "       -4.10282671e-01, -1.04298562e-01, -2.10569859e-01,  1.91073954e-01,\n",
       "        3.52574229e-01, -6.37128055e-02,  4.55014259e-01, -1.76426351e-01,\n",
       "       -1.53045595e-01, -3.31944287e-01, -1.47508010e-01, -2.38953382e-02,\n",
       "       -3.78697097e-01,  3.91504318e-01,  2.82836527e-01, -1.51635259e-02,\n",
       "       -3.14484000e-01, -3.78861099e-01,  8.70728120e-02, -3.06445658e-01,\n",
       "        4.27105188e-01,  2.67266124e-01,  1.88362777e-01,  5.17748535e-01,\n",
       "       -1.14594966e-01, -1.72126189e-01, -8.86903107e-02,  1.83915317e-01,\n",
       "        1.28115714e-02,  2.49523580e-01,  6.67933285e-01,  2.37140626e-01,\n",
       "        2.21194357e-01, -8.51178765e-02, -2.64454246e-01,  3.63297641e-01,\n",
       "       -3.34018677e-01,  5.54347783e-02,  1.41062215e-01, -4.24173594e-01,\n",
       "       -6.19544804e-01,  2.61383355e-01,  2.83092022e-01,  4.06241193e-02,\n",
       "        2.91692108e-01, -2.35585541e-01,  2.85905331e-01, -3.49394172e-01,\n",
       "        2.95804709e-01,  2.16423534e-02, -6.05200231e-02,  1.03005558e-01,\n",
       "       -2.28249788e-01, -3.50028068e-01, -1.38776943e-01, -1.33684605e-01,\n",
       "        1.48611456e-01,  1.50377646e-01, -1.05283335e-01, -1.94802657e-01,\n",
       "        5.07364869e-01, -1.35165542e-01, -4.72387113e-02,  7.38389716e-02,\n",
       "       -1.05174243e-01,  1.36998042e-01, -1.70494169e-01, -1.88505739e-01,\n",
       "        3.92484933e-01, -1.12603605e-02, -6.73037767e-02, -8.07203725e-02,\n",
       "       -2.52461523e-01,  9.48597491e-03,  7.58994222e-02, -1.58647209e-01,\n",
       "        1.13310382e-01,  4.49197814e-02, -5.48303202e-02,  4.51541543e-01,\n",
       "        2.90336132e-01,  3.61523986e-01,  1.75325200e-01, -7.59128332e-02,\n",
       "        1.77834824e-01, -3.78519833e-01, -8.73745605e-02,  6.64433479e-01,\n",
       "       -3.43521267e-01,  2.98503488e-02, -2.93209314e-01,  9.41160694e-03,\n",
       "       -2.07079723e-01, -1.87964126e-01, -3.11392248e-01, -7.13576823e-02,\n",
       "       -2.41012871e-01, -1.94302984e-02, -9.43707004e-02,  3.85558784e-01,\n",
       "       -1.55157283e-01, -1.23131558e-01, -2.19866037e-02, -4.34426486e-01,\n",
       "        2.98308879e-01, -4.22876567e-01,  3.12213123e-01, -1.81138709e-01,\n",
       "        3.12090814e-01,  4.63359416e-01,  2.76667267e-01,  2.13731110e-01,\n",
       "        2.73600429e-01, -4.76957887e-01,  2.78873116e-01,  2.12885030e-02,\n",
       "        3.83534245e-02,  3.93875241e-02, -2.48804450e-01,  4.44487572e-01,\n",
       "       -1.41265109e-01,  8.63472447e-02, -4.93074417e-01,  4.13041204e-01,\n",
       "       -3.25764678e-02, -2.94960942e-03,  2.30454728e-02,  3.87470752e-01,\n",
       "       -1.27549976e-01,  1.69974677e-02,  3.89935188e-02, -2.50117540e-01,\n",
       "        3.68043870e-01,  1.51041672e-01, -3.87392908e-01, -5.46396494e-01,\n",
       "        1.48946509e-01,  1.24877781e-01,  3.22173327e-01,  2.68144995e-01,\n",
       "        1.95356399e-01, -3.32733482e-01, -2.95268267e-01,  2.57781416e-01,\n",
       "        2.43500978e-01,  4.04016107e-01,  3.47293198e-01, -3.79056603e-01,\n",
       "        2.63342202e-01, -5.50162643e-02, -1.07720919e-01, -4.88280877e-03,\n",
       "       -3.42413113e-02,  3.47426772e-01, -1.40850276e-01,  2.30609953e-01,\n",
       "       -5.15600085e-01, -4.53946590e-01, -1.34185441e-02,  1.86648294e-01,\n",
       "        4.74342257e-01, -2.44376391e-01,  2.84341007e-01, -4.63437289e-01,\n",
       "        2.61258930e-02, -2.44471118e-01, -2.20702082e-01,  4.32430744e-01,\n",
       "        4.20542926e-01, -1.95156530e-01, -2.44385779e-01,  1.83968514e-01,\n",
       "        5.17196357e-01,  1.30311638e-01,  1.78761147e-02, -3.19862634e-01,\n",
       "        3.85713190e-01,  5.16236067e-01, -1.62841111e-01,  4.66194674e-02,\n",
       "        3.75804365e-01, -1.92477927e-01,  2.28239119e-01, -2.99608648e-01,\n",
       "        1.17295995e-01,  3.11964691e-01,  1.34587079e-01, -2.72759497e-02,\n",
       "        1.16963297e-01, -4.54525650e-01,  2.85074055e-01, -1.20238855e-01,\n",
       "       -9.78382528e-02, -2.66662270e-01,  4.47078764e-01, -4.11475539e-01,\n",
       "        2.52310216e-01, -1.70178600e-02, -2.36854367e-02,  1.17064580e-01,\n",
       "        2.28726268e-01,  4.61720638e-02,  4.60509658e-01,  7.59530216e-02,\n",
       "       -1.61420673e-01,  1.69848382e-01, -3.41782160e-02,  2.37416416e-01,\n",
       "       -2.80962199e-01,  2.84616679e-01,  2.76029676e-01,  8.59566182e-02,\n",
       "       -6.31619096e-01,  1.41786858e-02, -1.07186824e-01, -4.13847417e-02,\n",
       "       -5.66390753e-01, -1.08479853e+01, -2.15032697e-02, -1.12075277e-01,\n",
       "        3.70676577e-01, -4.81164932e-01,  2.16995224e-01,  1.50652111e-01,\n",
       "       -3.45468104e-01,  4.00499860e-03,  2.70850156e-02, -1.03713405e+00,\n",
       "       -2.92688936e-01, -1.89029276e-02,  3.38741630e-01, -2.28961229e-01,\n",
       "       -5.49745798e-01, -3.51943791e-01, -1.53263971e-01,  5.01295924e-01,\n",
       "        3.17829996e-01, -4.88735259e-01,  9.70093727e-01,  1.73287451e-01,\n",
       "        3.15822303e-01,  4.05614793e-01, -6.12429157e-02, -1.72953069e-01,\n",
       "        5.27078919e-02,  2.50077784e-01, -6.28302515e-01, -1.86506063e-01,\n",
       "        3.22884202e-01,  8.44220072e-02,  5.07220805e-01,  4.18752283e-01,\n",
       "       -7.49580085e-01, -2.24916279e-01,  3.38072032e-01,  1.46250665e-01,\n",
       "       -3.00867796e-01, -2.04641744e-02,  1.32505596e-02,  3.55269015e-02,\n",
       "        3.38018447e-01,  5.47207706e-02, -2.86222368e-01,  1.56909734e-01,\n",
       "       -1.75837624e+00,  1.99985504e-02,  7.82672942e-01,  2.90837854e-01,\n",
       "        4.27425206e-01,  8.99329066e-01, -4.10173610e-02,  8.73432308e-02,\n",
       "       -6.59283772e-02, -3.46096069e-01, -7.12766051e-02, -2.77235657e-01,\n",
       "       -6.31992444e-02, -1.72293596e-02, -2.37067863e-01,  6.37527525e-01,\n",
       "        1.64338797e-01, -4.32730131e-02,  3.30177486e-01, -1.08121514e-01,\n",
       "       -8.21119100e-02, -3.86665791e-01, -8.52298617e-01, -5.29253840e-01,\n",
       "       -2.89973728e-02, -2.33632326e-01,  8.39593560e-02,  4.45482075e-01,\n",
       "        5.54014504e-01, -4.36104387e-02,  7.15362787e-01,  4.71100435e-02,\n",
       "        2.59170920e-01, -1.65858194e-01, -1.00326829e-01,  3.95639926e-01,\n",
       "        1.57746121e-01,  4.49742109e-01, -5.04933059e-01, -1.46882504e-01,\n",
       "        4.99947608e-01,  4.38558638e-01, -2.33337477e-01, -6.33195102e-01,\n",
       "       -7.10699707e-02, -4.70587574e-02, -3.56351852e-01, -3.30747962e-01,\n",
       "       -2.03053206e-02, -5.19318640e-01,  2.61235416e-01, -2.63981372e-02,\n",
       "        8.84522870e-02,  3.86870682e-01, -5.91012985e-02,  4.60240580e-02,\n",
       "       -1.51521534e-01, -4.18603331e-01,  2.84804642e-01,  1.65558785e-01,\n",
       "       -5.59841767e-02,  7.39121586e-02,  2.09484637e-01, -4.18952480e-02,\n",
       "       -3.68254483e-02,  1.99443609e-01,  3.65925074e-01,  1.07578956e-01,\n",
       "        2.16583446e-01,  4.91018385e-01, -1.75796911e-01,  4.18904752e-01,\n",
       "       -2.23321393e-02,  2.35962272e-01, -8.02373961e-02,  9.98577327e-02,\n",
       "        2.88529359e-02, -3.94976661e-02, -1.02693766e-01,  5.35543740e-01,\n",
       "       -4.44857180e-01,  4.12030339e-01, -2.45100170e-01,  1.36378974e-01,\n",
       "        9.77017730e-02, -4.45283532e-01, -1.35560244e-01,  3.86440724e-01,\n",
       "       -3.89725119e-01, -3.05347502e-01,  2.80566275e-01, -2.67455041e-01,\n",
       "        2.99500167e-01,  1.53473273e-01,  2.39518613e-01,  3.20084929e-01,\n",
       "        2.72162169e-01,  2.84123067e-02, -3.91639650e-01, -6.06080443e-02,\n",
       "       -1.90138340e-01, -5.59848487e-01, -5.46066046e-01, -2.50939667e-01,\n",
       "       -7.70725980e-02, -2.88041711e-01,  1.35497749e-03,  4.79568392e-02,\n",
       "       -1.23907752e-01,  3.40759993e-01, -2.66509891e-01, -1.78071827e-01,\n",
       "        1.38031051e-01, -3.41721088e-01,  4.10131484e-01, -9.89707410e-02,\n",
       "       -7.05530941e-02, -9.72581580e-02,  7.26766437e-02,  2.83325344e-01,\n",
       "        3.21107283e-02, -4.32582080e-01, -2.55044878e-01, -4.75090951e-01,\n",
       "       -1.68699175e-01, -6.12350777e-02, -8.30013230e-02, -9.73379984e-02,\n",
       "       -4.19552445e-01, -1.18396543e-01,  1.45392731e-01, -3.69388670e-01,\n",
       "       -1.90745562e-01,  3.47371727e-01, -2.31890365e-01, -2.64952987e-01,\n",
       "       -7.42559850e-01,  1.05461925e-02, -3.57230186e-01, -1.26069397e-01,\n",
       "        3.66604716e-01, -5.76327443e-02,  8.75314921e-02, -2.42454022e-01,\n",
       "        1.89652413e-01, -4.92239028e-01,  2.91389376e-02,  4.66423124e-01,\n",
       "        1.28310695e-01,  2.85698980e-01, -8.56107175e-02, -4.96374369e-02,\n",
       "        6.47067785e-01,  1.02561109e-01,  1.85667321e-01, -6.20928556e-02,\n",
       "       -1.80166498e-01,  6.61438704e-02,  1.83621332e-01,  1.43318594e-01,\n",
       "       -3.03213954e-01, -7.94688910e-02,  2.69270658e-01, -2.51657903e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic['recommendation_embed'] = sythentic['recommendation'].apply(EmbedMedical.get_medical_embeddings())\n",
    "\n",
    "sythentic['dist_vector_1'] = \n",
    "sythentic['dist_vector_2'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To get a single vector representation for each text, you can:\n",
    "\n",
    "# 1. Take the embedding of the [CLS] token (the first token)\n",
    "cls_embeddings = embeddings[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "print(\"CLS Embeddings shape:\", cls_embeddings.shape)\n",
    "print(\"Example CLS Embedding:\", cls_embeddings[0, :5]) # Print the first 5 elements of the first embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create analogy helper function\n",
    "def analogy(word1: str, word2: str, word3: str, model: 'KeyedVectors' = google_news_vectors) -> 'pd.DataFrame':\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given word embedding model.\n",
    "    \n",
    "    Finds the word that completes the analogy relation word1:word2::word3:? using vector arithmetic\n",
    "    in the embedding space. For example, \"king:man::woman:?\" would return \"queen\" as the top result.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word1 : str\n",
    "        First word in the analogy relation (e.g. \"king\")\n",
    "    word2 : str  \n",
    "        Second worad in the analogy relation (e.g. \"man\")\n",
    "    word3 : str\n",
    "        Third word in the analogy relation (e.g. \"woman\") \n",
    "    model : gensim.models.KeyedVectors\n",
    "        Word embedding model containing the word vectors\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the top analogy words and their similarity scores\n",
    "        Columns are [\"Analogy word\", \"Score\"]\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect size: 1.7604\n",
      "P-value: 0.0000\n",
      "Mean association of X with male vs. female attributes: 0.0352\n",
      "Mean association of Y with male vs. female attributes: -0.1590\n"
     ]
    }
   ],
   "source": [
    "# create wefat test\n",
    "def weat_test(X, Y, A, B, model=google_news_vectors, n_samples=10000):\n",
    "    \"\"\"\n",
    "    Performs the Word Embedding Association Test (WEAT).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : list\n",
    "        First set of target words\n",
    "    Y : list\n",
    "        Second set of target words\n",
    "    A : list\n",
    "        First set of attribute words\n",
    "    B : list\n",
    "        Second set of attribute words\n",
    "    model : gensim.models.KeyedVectors\n",
    "        Word embedding model containing the word vectors\n",
    "    n_samples : int\n",
    "        Number of permutation samples for the p-value calculation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing test results:\n",
    "        - 'effect_size': Normalized measure of separation between distributions\n",
    "        - 'p_value': One-sided p-value from the permutation test\n",
    "        - 'association_X': Mean association scores for words in X\n",
    "        - 'association_Y': Mean association scores for words in Y\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Check if words are in vocabulary\n",
    "    all_words = X + Y + A + B\n",
    "    missing_words = [word for word in all_words if word not in model.key_to_index]\n",
    "    if missing_words:\n",
    "        print(f\"Warning: The following words are not in the model vocabulary: {missing_words}\")\n",
    "        \n",
    "    # Filter out missing words\n",
    "    X = [x for x in X if x in model.key_to_index]\n",
    "    Y = [y for y in Y if y in model.key_to_index]\n",
    "    A = [a for a in A if a in model.key_to_index]\n",
    "    B = [b for b in B if b in model.key_to_index]\n",
    "    \n",
    "    if not (X and Y and A and B):\n",
    "        raise ValueError(\"After filtering, at least one word set is empty\")\n",
    "    \n",
    "    # Function to calculate association of a word with attributes\n",
    "    def s(w, A, B):\n",
    "        \"\"\"\n",
    "        Measures association of word w with attribute sets A and B\n",
    "        s(w, A, B) = mean_{a∈A}cos(w,a) - mean_{b∈B}cos(w,b)\n",
    "        \"\"\"\n",
    "        return np.mean([model.similarity(w, a) for a in A]) - np.mean([model.similarity(w, b) for b in B])\n",
    "    \n",
    "    # Calculate association scores for all target words\n",
    "    x_scores = [s(x, A, B) for x in X]\n",
    "    y_scores = [s(y, A, B) for y in Y]\n",
    "    w_scores = x_scores + y_scores\n",
    "    \n",
    "    # Calculate observed test statistic\n",
    "    test_statistic = sum(x_scores) - sum(y_scores)\n",
    "    \n",
    "    # Calculate effect size\n",
    "    effect_size = (np.mean(x_scores) - np.mean(y_scores)) / np.std(w_scores, ddof=1)\n",
    "    \n",
    "    # Permutation test\n",
    "    target_words = X + Y\n",
    "    count = 0\n",
    "    for _ in range(n_samples):\n",
    "        np.random.shuffle(target_words)\n",
    "        Xi = target_words[:len(X)]\n",
    "        Yi = target_words[len(X):]\n",
    "        xi_scores = [s(x, A, B) for x in Xi]\n",
    "        yi_scores = [s(y, A, B) for y in Yi]\n",
    "        sample_test_statistic = sum(xi_scores) - sum(yi_scores)\n",
    "        if sample_test_statistic > test_statistic:\n",
    "            count += 1\n",
    "    \n",
    "    p_value = count / n_samples\n",
    "    \n",
    "    return {\n",
    "        'effect_size': effect_size,\n",
    "        'p_value': p_value,\n",
    "        'association_X': np.mean(x_scores),\n",
    "        'association_Y': np.mean(y_scores)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# Testing for gender bias in occupation words\n",
    "X = [\"programmer\", \"engineer\", \"scientist\", \"developer\", \"mathematician\"]  # Stereotypically male\n",
    "Y = [\"nurse\", \"teacher\", \"librarian\", \"receptionist\", \"homemaker\"]  # Stereotypically female\n",
    "A = [\"man\", \"male\", \"he\", \"him\", \"his\"]  # Male attributes\n",
    "B = [\"woman\", \"female\", \"she\", \"her\", \"hers\"]  # Female attributes\n",
    "\n",
    "results = weat_test(X, Y, A, B)\n",
    "print(f\"Effect size: {results['effect_size']:.4f}\")\n",
    "print(f\"P-value: {results['p_value']:.4f}\")\n",
    "print(f\"Mean association of X with male vs. female attributes: {results['association_X']:.4f}\")\n",
    "print(f\"Mean association of Y with male vs. female attributes: {results['association_Y']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
